#!/usr/bin/env python3
"""
LangChain å…¼å®¹çš„ Qwen API å®¢æˆ·ç«¯ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LangChain è°ƒç”¨éƒ¨ç½²çš„ Qwen æ¨¡å‹
"""

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

def example_langchain_basic():
    """ç¤ºä¾‹1: åŸºç¡€ LangChain è°ƒç”¨"""
    print("=" * 70)
    print("ç¤ºä¾‹1: åŸºç¡€ LangChain è°ƒç”¨")
    print("=" * 70)
    
    # åˆå§‹åŒ– LangChain ChatOpenAI å®¢æˆ·ç«¯
    # æœåŠ¡å™¨å·²ä½¿ç”¨ --served-model-nameï¼Œå¯ä»¥ä½¿ç”¨ç®€çŸ­åç§°
    llm = ChatOpenAI(
        model="Qwen-32B-Novel",  # ä½¿ç”¨ç®€çŸ­åç§°ï¼ˆæœåŠ¡å™¨å·²é…ç½®ï¼‰
        base_url="http://122.193.22.114:8888/v1",  # vLLM API ç«¯ç‚¹ï¼ˆæ³¨æ„ /v1 åç¼€ï¼‰
        api_key="sk-6tT86nzygIVWl0naxnWo8SjI4ClTSzYl05nppF9sYuY",  # ä½ çš„ API key
        temperature=0.7,
        max_tokens=500,
        timeout=300,  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    )
    
    # å‘é€æ¶ˆæ¯
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´å†™ä½œåŠ©æ‰‹ã€‚"),
        HumanMessage(content="å†™ä¸€ä¸ªç§‘å¹»å°è¯´çš„å¼€å¤´ï¼Œå¤§çº¦200å­—")
    ]
    
    try:
        response = llm.invoke(messages)
        print(f"\nå“åº”: {response.content}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def example_langchain_streaming():
    """ç¤ºä¾‹2: æµå¼è¾“å‡º"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹2: æµå¼è¾“å‡º")
    print("=" * 70)
    
    llm = ChatOpenAI(
        model="Qwen-32B-Novel",  # ä½¿ç”¨ç®€çŸ­åç§°
        base_url="http://localhost:8888/v1",
        api_key="sk-6tT86nzygIVWl0naxnWo8SjI4ClTSzYl05nppF9sYuY",
        temperature=0.7,
        max_tokens=500,
        streaming=True,  # å¯ç”¨æµå¼è¾“å‡º
        timeout=300,
    )
    
    messages = [
        HumanMessage(content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")
    ]
    
    try:
        print("æµå¼è¾“å‡º:\n")
        for chunk in llm.stream(messages):
            if chunk.content:
                print(chunk.content, end='', flush=True)
        print("\n")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def example_langchain_async():
    """ç¤ºä¾‹3: å¼‚æ­¥è°ƒç”¨"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹3: å¼‚æ­¥è°ƒç”¨")
    print("=" * 70)
    
    import asyncio
    
    llm = ChatOpenAI(
        model="Qwen-32B-Novel",  # ä½¿ç”¨ç®€çŸ­åç§°
        base_url="http://localhost:8888/v1",
        api_key="sk-6tT86nzygIVWl0naxnWo8SjI4ClTSzYl05nppF9sYuY",
        temperature=0.7,
        max_tokens=500,
        timeout=300,
    )
    
    async def async_call():
        messages = [
            HumanMessage(content="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
        ]
        
        try:
            response = await llm.ainvoke(messages)
            print(f"\nå“åº”: {response.content}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    asyncio.run(async_call())


def example_langchain_chain():
    """ç¤ºä¾‹4: ä½¿ç”¨ LangChain Chain"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹4: ä½¿ç”¨ LangChain Chain")
    print("=" * 70)
    
    from langchain.chains import LLMChain
    from langchain.prompts import ChatPromptTemplate
    
    llm = ChatOpenAI(
        model="Qwen-32B-Novel",  # ä½¿ç”¨ç®€çŸ­åç§°
        base_url="http://localhost:8888/v1",
        api_key="sk-6tT86nzygIVWl0naxnWo8SjI4ClTSzYl05nppF9sYuY",
        temperature=0.7,
        max_tokens=500,
        timeout=300,
    )
    
    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´å†™ä½œåŠ©æ‰‹ã€‚"),
        ("human", "{input}")
    ])
    
    # åˆ›å»ºé“¾
    chain = LLMChain(llm=llm, prompt=prompt)
    
    try:
        result = chain.invoke({"input": "å†™ä¸€ä¸ªå…³äºæ—¶é—´æ—…è¡Œçš„æ•…äº‹å¼€å¤´"})
        print(f"\nå“åº”: {result['text']}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def example_langchain_with_custom_config():
    """ç¤ºä¾‹5: è‡ªå®šä¹‰é…ç½®ï¼ˆä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹5: ä»é…ç½®åŠ è½½")
    print("=" * 70)
    
    import json
    from pathlib import Path
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆç±»ä¼¼ä½ çš„é…ç½®æ ¼å¼ï¼‰
    config = {
        "name": "Qwen-32B-Novel",
        "base_url": "http://localhost:8888",
        "model": "Qwen-32B-Novel",
        "api_key": "sk-6tT86nzygIVWl0naxnWo8SjI4ClTSzYl05nppF9sYuY",
        "supports_thinking": False,
        "max_retries": 3
    }
    
    # åˆå§‹åŒ– LangChain å®¢æˆ·ç«¯
    # æ³¨æ„ï¼šbase_url éœ€è¦æ·»åŠ  /v1 åç¼€
    # åˆå§‹åŒ– LangChain å®¢æˆ·ç«¯
    # æ³¨æ„ï¼šbase_url éœ€è¦æ·»åŠ  /v1 åç¼€
    base_url = config['base_url']
    if not base_url.endswith('/v1'):
        base_url = f"{base_url}/v1"
    
    llm = ChatOpenAI(
        model=config["model"],  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
        base_url=base_url,
        api_key=config["api_key"],
        temperature=0.7,
        max_tokens=500,
        timeout=300,
        max_retries=config.get("max_retries", 3),
    )
    
    messages = [
        HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
    ]
    
    try:
        response = llm.invoke(messages)
        print(f"\nå“åº”: {response.content}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nğŸ’¡ å¸¸è§é—®é¢˜æ’æŸ¥:")
        print("   1. æ£€æŸ¥ base_url æ˜¯å¦æ­£ç¡®ï¼ˆéœ€è¦åŒ…å« /v1 åç¼€ï¼‰")
        print("   2. æ£€æŸ¥ API key æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ: curl http://122.193.22.114:8888/health")
        print("   4. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®: Qwen-32B-Novel æˆ– Qwen-32B-Instruct")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
    
    # ç¤ºä¾‹1: åŸºç¡€è°ƒç”¨
    example_langchain_basic()
    
    # ç¤ºä¾‹2: æµå¼è¾“å‡º
    # example_langchain_streaming()
    
    # ç¤ºä¾‹3: å¼‚æ­¥è°ƒç”¨
    # example_langchain_async()
    
    # ç¤ºä¾‹4: ä½¿ç”¨ Chain
    # example_langchain_chain()
    
    # ç¤ºä¾‹5: ä»é…ç½®åŠ è½½
    # example_langchain_with_custom_config()

